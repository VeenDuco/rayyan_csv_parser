{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data Kennisinstituut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggested improvements:\n",
    "* Retrieve missing doi links (can be done using [this script](https://github.com/asreview/synergy-dataset/blob/461a0f757439c226acbc6bc320359001ecd26c69/scripts/enrich.py))\n",
    "* Anonymize the experts and report all inclusions and exclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction from csv files\n",
    "\n",
    "In this part we will extract data from the csv files that are exported from rayyan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load in the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np # for np.nan\n",
    "import re # for searching in notes column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all csv files from the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all .csv files in the current directory\n",
    "csv_files = [file for file in os.listdir('.') if file.endswith('.csv')]\n",
    "\n",
    "# Display the list of .csv files\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out how many colums each file has (are they equal? -> No). Then find the common column names so we can extract the relevant data from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = [] # number of columns in each file\n",
    "column_sets = []  # List to store the set of columns from each file\n",
    "\n",
    "for file_name in csv_files:\n",
    "    df = pd.read_csv(file_name, sep=';', engine='python')\n",
    "    ncols.append(df.shape[1]) # Append the number of columns to the list\n",
    "    \n",
    "    # Append the set of column names to the list\n",
    "    column_sets.append(set(df.columns))\n",
    "\n",
    "print(ncols)\n",
    "\n",
    "# Find columns present in all files\n",
    "common_columns = set.intersection(*column_sets)\n",
    "print(\"Columns present in all files:\", common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in a dataset\n",
    "In the final script, this will be done in a loop. This document is there to explain what is going on. So for now, load in a single dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file with semicolon separator\n",
    "df = pd.read_csv(csv_files[0], sep=';', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for inclusion status\n",
    "\n",
    "The inclusion status, that is the labels provided by the experts, can usualy be found in the notes column. However, for some rows in some documents this information seems to have jumped around somehow. Therefore, for each line we need to evaluate if the relevant information present, and where. Then we should store it in a new column.\n",
    "\n",
    "The patern that we look for goes like this: \n",
    "RAYYAN-INCLUSION: {\"\"RATER 1\"\"=>\"\"Excluded\"\", \"\"RATER 2\"\"=>\"\"Excluded\"\"}\n",
    "\n",
    "So we define a function to look for the patern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search all columns in a row for inclusion status\n",
    "def find_inclusion_status_in_row(row):\n",
    "    for col in row.index:\n",
    "        value = str(row[col])\n",
    "        if pd.notnull(value):\n",
    "            match = re.search(r'RAYYAN-INCLUSION:\\s*({.*?})', value)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can be used to create a new column on `inclusion_status` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row to find inclusion_status\n",
    "df['inclusion_status'] = df.apply(find_inclusion_status_in_row, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to extract the inclusion status provided by each expert and map them to a final decision. For that we use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map decisions to codes\n",
    "def map_decision(decision):\n",
    "    if decision.lower() == \"excluded\":\n",
    "        return 0\n",
    "    elif decision.lower() == \"included\":\n",
    "        return 1\n",
    "    elif decision.lower() == \"maybe\":\n",
    "        return 999\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to extract names and coded decisions from inclusion_status\n",
    "def extract_decisions(inclusion_status):\n",
    "    if pd.isnull(inclusion_status):\n",
    "        return None\n",
    "    decisions = re.findall(r'\"(.*?)\"\\s*=>\\s*\"(.*?)\"', inclusion_status)\n",
    "    return {name: map_decision(decision) for name, decision in decisions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be applied by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'inclusion_status' column\n",
    "df['coded_decisions'] = df['inclusion_status'].apply(extract_decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the TI-AB label is created by means of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TI-AB column based on the coded_decisions\n",
    "df['TI-AB'] = df['coded_decisions'].apply(\n",
    "    lambda decisions: np.nan if decisions is None or decisions == 'None' else (\n",
    "        0 if decisions and all(decision == 0 for decision in decisions.values()) else 1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, if all experts agree to exclude, exclude. If any experts does not agree, move it on the the next phase of screening and include it in the title-abstract screening phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing irrelevant columns\n",
    "Not all the common columns are relevant so after looking into the content we remove some columns that we don't need to select.\n",
    "\n",
    "Columns from the original data that are removed are:\n",
    "- authors\n",
    "- issue\n",
    "- key\n",
    "- language\n",
    "- month\n",
    "- volume\n",
    "- publisher\n",
    "- journal\n",
    "- issn\n",
    "- pages\n",
    "- day\n",
    "- pmc_id\n",
    "- location\n",
    "- year\n",
    "\n",
    "Or phrased differently, we keep the original columns:\n",
    "\n",
    "- title\n",
    "- abstract\n",
    "- pubmed_id\n",
    "- url \n",
    "\n",
    "and the created column\n",
    "- TI-AB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting doi links where possible\n",
    "\n",
    "The url column contains links to the relevant papers. These can be doi link or others. We want to get all the DOI links and use a function to get them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract DOI link from a string\n",
    "def extract_doi(url_string):\n",
    "    if pd.isnull(url_string):\n",
    "        return None\n",
    "    # Regular expression pattern to match DOI URLs\n",
    "    doi_pattern = r'(https?://(?:dx\\.)?doi\\.org/[^\\s]+)'\n",
    "    matches = re.findall(doi_pattern, url_string)\n",
    "    if matches:\n",
    "        # Return the first DOI link found\n",
    "        return matches[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Regular Expression Breakdown:**\n",
    "  - `https?://` matches `http://` or `https://`.\n",
    "  - `(?:dx\\.)?` matches `dx.` if present; the `?` makes it optional.\n",
    "  - `doi\\.org/` matches `doi.org/`.\n",
    "  - `[^\\s]+` matches one or more non-whitespace characters (the DOI identifier).\n",
    "  - The parentheses `()` capture the entire DOI URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the code to get a `doi` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract DOI links from the 'url' column\n",
    "df['doi'] = df['url'].apply(extract_doi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can now drop the `url` and `pubmed_id` columns and only include the `doi` column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevan columns\n",
    "columns = ['title', 'abstract', 'doi', \"TI-AB\"]\n",
    "# select the relevant columns\n",
    "df_selected = df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean export\n",
    "\n",
    "Now we need to cleanup the 'df_selected' DataFrame and export the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the df_selected DataFrame to a CSV file with the modified name\n",
    "output_file_name = file_name.replace('.csv', '_CLEAN.csv')\n",
    "# prepend output_file_name with 'TRAM_'\n",
    "output_file_name = 'TRAM_' + output_file_name\n",
    "df_selected.to_csv(output_file_name, index=False, sep=\";\")\n",
    "\n",
    "# Display the name of the output file\n",
    "print(f\"DataFrame exported to {output_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
